#!/usr/bin/python3

import sounddevice
import speech_recognition as sr
from llama_cpp import Llama
import pyttsx4
from threaded_buffered_pipeline import buffered_pipeline
import threading
from tacotron2.text import text_to_sequence
import models
import torch
import argparse
import numpy as np
from scipy.io.wavfile import write
from waveglow.denoiser import Denoiser
from io import StringIO
import wave
import time
import pyaudio

def checkpoint_from_distributed(state_dict):
	"""
	Checks whether checkpoint was generated by DistributedDataParallel. DDP
	wraps model in additional "module.", it needs to be unwrapped for single
	GPU inference.
	:param state_dict: model's state dict
	"""
	ret = False
	for key, _ in state_dict.items():
		if key.find('module.') != -1:
			ret = True
			break
	return ret


def unwrap_distributed(state_dict):
	"""
	Unwraps model from DistributedDataParallel.
	DDP wraps model in additional "module.", it needs to be removed for single
	GPU inference.
	:param state_dict: model's state dict
	"""
	new_state_dict = {}
	for key, value in state_dict.items():
		new_key = key.replace('module.', '')
		new_state_dict[new_key] = value
	return new_state_dict


def load_and_setup_model(model_name, parser, checkpoint, fp16_run, cpu_run,
						 forward_is_infer=False, jittable=False):
	model_parser = models.model_parser(model_name, parser, add_help=False)
	model_args, _ = model_parser.parse_known_args()

	model_config = models.get_model_config(model_name, model_args)
	model = models.get_model(model_name, model_config, cpu_run=cpu_run,
							 forward_is_infer=forward_is_infer,
							 jittable=jittable)

	if checkpoint is not None:
		if cpu_run:
			state_dict = torch.load(checkpoint, map_location=torch.device('cpu'))['state_dict']
		else:
			state_dict = torch.load(checkpoint)['state_dict']
		if checkpoint_from_distributed(state_dict):
			state_dict = unwrap_distributed(state_dict)

		model.load_state_dict(state_dict)

	if model_name == "WaveGlow":
		model = model.remove_weightnorm(model)

	model.eval()

	if fp16_run:
		model.half()

	return model


# taken from tacotron2/data_function.py:TextMelCollate.__call__
def pad_sequences(batch):
	# Right zero-pad all one-hot text sequences to max input length
	input_lengths, ids_sorted_decreasing = torch.sort(
		torch.LongTensor([len(x) for x in batch]),
		dim=0, descending=True)
	max_input_len = input_lengths[0]

	text_padded = torch.LongTensor(len(batch), max_input_len)
	text_padded.zero_()
	for i in range(len(ids_sorted_decreasing)):
		text = batch[ids_sorted_decreasing[i]]
		text_padded[i, :text.size(0)] = text

	return text_padded, input_lengths


def prepare_input_sequence(texts):
	d = []
	for i,text in enumerate(texts):
		d.append(torch.IntTensor(
			text_to_sequence(text, ['english_cleaners'])[:]))

	text_padded, input_lengths = pad_sequences(d)
	text_padded = text_padded.long()
	input_lengths = input_lengths.long()

	return text_padded, input_lengths




state = 0
cond = threading.Condition()

def listen():
	global cond
	global state

	recognizer = sr.Recognizer()
	microphone = sr.Microphone()

	while True:
		with cond:
			while state != 0:
				cond.wait()

		with microphone as source:
			print("Adjusting for noise...")
			recognizer.adjust_for_ambient_noise(source)

		ready = False
		while not ready:
			with microphone as source:
				try:
					print("Listening...")
					audio = recognizer.listen(source, timeout=3, phrase_time_limit=8)
					ready = True
				except sr.WaitTimeoutError:
					pass

		try:
			print("Recognizing speech...")
			text = recognizer.recognize_whisper(audio)
			if text and text != " Thank you.":
				yield text[1:]
				print("I heard \"" + text[1:] + "\"")
				with cond:
					state = 1
					cond.notify()
		except sr.RequestError:
			pass
		except sr.UnknownValueError:
			pass
		
def respond(texts,
	max_tokens=256,
	temperature=0.1,
	top_p=0.5,
	echo=False,
	stop=["\""]):
	llm = Llama(model_path="ggml-vicuna-7b-1.1-q4_1.bin", n_ctx=512, n_batch=126, last_n_tokens_size=1024, verbose=False)
	prompt = "Lira is listening to a conversation between her friends, and occationally interjects to provide helpful suggestions using simple and concise language. "
	buffer = ""
	code = False
	for text in texts:
		prompt += "They say, \"" + text + "\" Lira replies \"" 
		stream = llm(
			prompt,
			max_tokens=max_tokens,
			temperature=temperature,
			top_p=top_p,
			echo=echo,
			stop=stop,
			stream=True)
		prompt = ""
		for resp in stream:
			pos = len(buffer)
			buffer += resp["choices"][0]["text"]
			while pos < len(buffer):
				if not code and buffer[pos] in "\n.!?":
					break
				elif code and buffer[pos] in "\n:;":
					break
				elif buffer[pos] == "`":
					code = not code
				pos += 1

			if pos < len(buffer):
				yield buffer[0:pos+1]
				buffer = buffer[pos+1:]
		if len(buffer) > 0:
			yield buffer
			buffer = ""
		yield ""

def speak(texts, parser):
	tacotron2 = load_and_setup_model('Tacotron2', parser, "tacotronModel", False, True, forward_is_infer=True)
	waveglow = load_and_setup_model('WaveGlow', parser, "waveglowModel", False, True, forward_is_infer=True, jittable=True)

	dn = Denoiser(waveglow)
	waveglow.make_ts_scriptable()
	wg = torch.jit.script(waveglow)
	t2 = torch.jit.script(tacotron2)

	buffer = ""
	for text in texts:
		if len(text) == 0:
			yield ""
		else:
			buffer += text
			hasSpeakable = False
			for c in buffer:
				if c.isalnum():
					hasSpeakable = True
					break

			if hasSpeakable:
				print("\nStarting inference")
				sequences_padded, input_lengths = prepare_input_sequence([buffer])
				with torch.no_grad():
					mel, mel_lengths, alignments = t2(sequences_padded, input_lengths)
					audios = wg(mel, sigma=0.9)
					audios = audios.float()
					audios = dn(audios, strength=0.01).squeeze(1)

				for i, audio in enumerate(audios):
					audio = audio[:mel_lengths[i]*256]
					audio = audio/torch.max(torch.abs(audio))
					yield audio.cpu().numpy().tobytes()
				print("Done")
				buffer = ""

def play(audios):
	global cond
	global state
	
	aud = pyaudio.PyAudio()
	
	for audio in audios:
		if len(audio) == 0:
			with cond:
				while state != 1:
					cond.wait()
				state = 0
				cond.notify()
		else:
			with cond:
				if state == 1:
					numChannels = 1
					sampleRate = 22050
					stream = aud.open(format=pyaudio.paFloat32, channels=numChannels, rate=sampleRate, output=True)
					stream.write(audio)
					stream.stop_stream()  
					stream.close()

	aud.terminate()
 
if __name__ == "__main__":
	parser = argparse.ArgumentParser(description='PyTorch Tacotron 2 Inference')

	buffer_iterable = buffered_pipeline()
	text = buffer_iterable(listen())
	output = buffer_iterable(respond(text))
	audios = buffer_iterable(speak(output, parser))
	play(audios)
